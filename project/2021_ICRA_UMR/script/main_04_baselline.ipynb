{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines (ALE and LWL$^2$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "from swae import SharedWassersteinAutoEncoderClass\n",
    "from util import *\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'    \n",
    "print (\"Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select which robot model to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name:[coman]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'coman'\n",
    "print(\"model_name:[%s]\"%(model_name)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11758] glue data from [100] motion retargeting data.\n",
      "[200004] robot-specific data.\n",
      "[201123] mocap-specific data.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "PRINT_EACH_MAT = 0\n",
    "\n",
    "# 2. Paired data from motion retargeting \n",
    "math_paths = glob.glob('../data/Glue of CMU mocap*%s.mat'%(model_name))\n",
    "math_paths = math_paths[:100] # use upto 100\n",
    "for m_idx,mat_path in enumerate(math_paths):\n",
    "    l = sio.loadmat(mat_path) # load\n",
    "    if m_idx == 0:\n",
    "        q_glue_tildes,q_glue_bars,x_glue_tildes,x_glue_bars = \\\n",
    "            l['q_tildes'],l['q_bars'],l['x_tildes'],l['x_bars']\n",
    "    else:\n",
    "        gamma = 0 \n",
    "        q_glue_tildes = np.concatenate((q_glue_tildes,l['q_tildes']),axis=0)\n",
    "        q_glue_bars = np.concatenate((q_glue_bars,l['q_bars']),axis=0)\n",
    "        x_glue_tildes = np.concatenate((x_glue_tildes,gamma*l['x_bars']+(1-gamma)*l['x_tildes']),axis=0)\n",
    "        x_glue_bars = np.concatenate((x_glue_bars,l['x_bars']),axis=0)\n",
    "print (\"[%d] glue data from [%d] motion retargeting data.\"%(q_glue_bars.shape[0],len(math_paths)))  \n",
    "\n",
    "# 3. Robot-specific data\n",
    "math_paths = glob.glob('../data/Domain of %s*.mat'%(model_name))\n",
    "for m_idx,mat_path in enumerate(math_paths):\n",
    "    l = sio.loadmat(mat_path) # load\n",
    "    if m_idx == 0:\n",
    "        q_domain_bars = l['q_bars']\n",
    "    else:\n",
    "        q_domain_bars = np.concatenate((q_domain_bars,l['q_bars']),axis=0)\n",
    "print (\"[%d] robot-specific data.\"%(q_domain_bars.shape[0]))  \n",
    "\n",
    "# 4. Mocap-specific data\n",
    "math_paths = glob.glob('../data/Domain of %s*.mat'%('mocap'))\n",
    "for m_idx,mat_path in enumerate(math_paths):\n",
    "    l = sio.loadmat(mat_path) # load\n",
    "    if m_idx == 0:\n",
    "        x_domain_tildes = l['x_tildes']\n",
    "    else:\n",
    "        x_domain_tildes = np.concatenate((x_domain_tildes,l['x_tildes']),axis=0)\n",
    "print (\"[%d] mocap-specific data.\"%(x_domain_tildes.shape[0]))  \n",
    "\n",
    "print (\"Done.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_x_recon:[23516] n_x_latent:[224639] n_y_recon:[23516] n_y_latent:[223520] n_glue:[11758] n_x2y:[11758].\n"
     ]
    }
   ],
   "source": [
    "x_recon_in  = np.concatenate((x_glue_tildes,x_glue_bars),axis=0)\n",
    "x_recon_out = np.concatenate((x_glue_bars,x_glue_bars),axis=0)\n",
    "y_recon_in  = np.concatenate((q_glue_tildes,q_glue_bars),axis=0)\n",
    "y_recon_out = np.concatenate((q_glue_bars,q_glue_bars),axis=0)\n",
    "\n",
    "# For latent modeling, we use all possible data\n",
    "x_latent = np.concatenate((x_glue_tildes,x_glue_bars,x_domain_tildes),axis=0)\n",
    "y_latent = np.concatenate((q_glue_tildes,q_glue_bars,q_domain_bars),axis=0)\n",
    "\n",
    "# Relaxed mocap -> Feasible robot pose\n",
    "x_glue = x_glue_tildes\n",
    "y_glue = q_glue_bars\n",
    "\n",
    "# Relaxed mocap -> Feasible robot pose\n",
    "x_x2y = x_glue_tildes\n",
    "y_x2y = q_glue_bars\n",
    "\n",
    "# Print out stats\n",
    "n_x_recon,n_x_latent,n_y_recon,n_y_latent = x_recon_in.shape[0],x_latent.shape[0],y_recon_in.shape[0],y_latent.shape[0]\n",
    "n_glue,n_x2y = x_glue.shape[0],x_x2y.shape[0]\n",
    "print (\"n_x_recon:[%d] n_x_latent:[%d] n_y_recon:[%d] n_y_latent:[%d] n_glue:[%d] n_x2y:[%d].\"%\n",
    "       (n_x_recon,n_x_latent,n_y_recon,n_y_latent,n_glue,n_x2y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xdim:[21] ydim:[11].\n"
     ]
    }
   ],
   "source": [
    "xdim,ydim = x_glue_tildes.shape[1],q_glue_bars.shape[1]\n",
    "print ('xdim:[%d] ydim:[%d].'%(xdim,ydim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ($\\texttt{MR}: x \\mapsto q$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "seed = 0\n",
    "zdim = 14 # max(14,qdim)\n",
    "hdims,actv_Q,actv_P,actv_D = [128]*3,tf.nn.relu,tf.nn.relu,tf.nn.relu\n",
    "# ki = tf.contrib.layers.xavier_initializer()\n",
    "ki = tf.truncated_normal_initializer(stddev=0.1) \n",
    "adam_beta1,adam_beta2,adam_epsilon = 0.9,0.9,1e-0 # 0.5,0.9,1e-0\n",
    "max_iter,batch_size,print_every,save_every = 1e5,64,1000,1000\n",
    "lr_rate_fr,lr_rate_to,warmup_it = 1.0,0.1,5e3\n",
    "# Latent prior\n",
    "latent_beta = 0.1 # 0.1\n",
    "lr_d = 2e-4\n",
    "lr_g = 2e-4\n",
    "# WAE recon\n",
    "l1_recon_coef = 0.0\n",
    "l2_recon_coef = 1.0 # 1.0\n",
    "lr_recon = 1e-3\n",
    "# Weight decay\n",
    "wd_coef = 1e-6\n",
    "# Latent consensus\n",
    "l1_lc_coef = 1.0\n",
    "l2_lc_coef = 5.0 # 1.0\n",
    "lr_lc = 1e-3\n",
    "# NCE SSL\n",
    "nce_coef = 0.01\n",
    "lr_nce = 1e-3\n",
    "# X->Y mapping\n",
    "l1_x2y_coef = 0.0\n",
    "l2_x2y_coef = 0.1\n",
    "lr_x2y = 1e-3\n",
    "    \n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /media/sj/SanDisk 500/Dropbox/Research/github/yart/project/2021_ICRA_UMR/script/wae.py:442: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /media/sj/SanDisk 500/Dropbox/Research/github/yart/project/2021_ICRA_UMR/script/wae.py:455: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /media/sj/SanDisk 500/Dropbox/Research/github/yart/project/2021_ICRA_UMR/script/wae.py:466: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /home/sj/.local/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/sj/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /media/sj/SanDisk 500/Dropbox/Research/github/yart/project/2021_ICRA_UMR/script/wae.py:554: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /media/sj/SanDisk 500/Dropbox/Research/github/yart/project/2021_ICRA_UMR/script/wae.py:567: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /media/sj/SanDisk 500/Dropbox/Research/github/yart/project/2021_ICRA_UMR/script/util.py:166: The name tf.losses.softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From /media/sj/SanDisk 500/Dropbox/Research/github/yart/project/2021_ICRA_UMR/script/util.py:40: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /media/sj/SanDisk 500/Dropbox/Research/github/yart/project/2021_ICRA_UMR/script/util.py:42: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(seed=seed); np.random.seed(seed=seed)\n",
    "S = SharedWassersteinAutoEncoderClass(\n",
    "    xname='lwl2_wae_x',yname='lwl2_wae_y',xdim=xdim,ydim=ydim,zdim=zdim,\n",
    "    hdims_Q=hdims,hdims_P=hdims,hdims_D=hdims,\n",
    "    actv_Q=tf.nn.relu,actv_P=tf.nn.relu,actv_D=tf.nn.relu,\n",
    "    actv_latent=None,actv_out=None,ki=ki,\n",
    "    adam_beta1=adam_beta1,adam_beta2=adam_beta2,adam_epsilon=adam_epsilon,\n",
    ")\n",
    "sess = gpu_sess() \n",
    "tf.set_random_seed(seed=seed); np.random.seed(seed=seed)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0][0.0%]lr:[0.00] X R:[7.005] D:[0.069] G:[0.069] WD:[0.000] / Y R:[7.084] D:[0.069] G:[0.070] WD:[0.000]\n",
      "   LC:[6.018] / NCE:[0.000] / X2Y:[0.685] / total_loss:[21.069]\n",
      "[nets/lwl2_wae_x/weights.mat] saved. Size is[0.448]MB.\n",
      "[nets/lwl2_wae_y/weights.mat] saved. Size is[0.436]MB.\n",
      "Checkpoint it:[0] total_loss:[21.069].\n",
      "[1000][1.0%]lr:[0.20] X R:[1.698] D:[0.069] G:[0.070] WD:[0.000] / Y R:[1.980] D:[0.070] G:[0.068] WD:[0.000]\n",
      "   LC:[0.405] / NCE:[0.000] / X2Y:[0.183] / total_loss:[4.542]\n",
      "[nets/lwl2_wae_x/weights.mat] saved. Size is[0.448]MB.\n",
      "[nets/lwl2_wae_y/weights.mat] saved. Size is[0.436]MB.\n",
      "Checkpoint it:[1000] total_loss:[4.542].\n",
      "[2000][2.0%]lr:[0.40] X R:[1.191] D:[0.069] G:[0.071] WD:[0.000] / Y R:[0.914] D:[0.070] G:[0.068] WD:[0.000]\n",
      "   LC:[0.593] / NCE:[0.000] / X2Y:[0.226] / total_loss:[3.201]\n",
      "[nets/lwl2_wae_x/weights.mat] saved. Size is[0.448]MB.\n",
      "[nets/lwl2_wae_y/weights.mat] saved. Size is[0.436]MB.\n",
      "Checkpoint it:[2000] total_loss:[3.201].\n",
      "[3000][3.0%]lr:[0.60] X R:[0.515] D:[0.066] G:[0.076] WD:[0.000] / Y R:[0.308] D:[0.070] G:[0.068] WD:[0.000]\n",
      "   LC:[0.254] / NCE:[0.000] / X2Y:[0.210] / total_loss:[1.568]\n",
      "[nets/lwl2_wae_x/weights.mat] saved. Size is[0.448]MB.\n",
      "[nets/lwl2_wae_y/weights.mat] saved. Size is[0.436]MB.\n",
      "Checkpoint it:[3000] total_loss:[1.568].\n",
      "[4000][4.0%]lr:[0.80] X R:[0.437] D:[0.062] G:[0.086] WD:[0.000] / Y R:[0.409] D:[0.067] G:[0.074] WD:[0.000]\n",
      "   LC:[0.168] / NCE:[0.000] / X2Y:[0.171] / total_loss:[1.476]\n",
      "[nets/lwl2_wae_x/weights.mat] saved. Size is[0.448]MB.\n",
      "[nets/lwl2_wae_y/weights.mat] saved. Size is[0.436]MB.\n",
      "Checkpoint it:[4000] total_loss:[1.476].\n",
      "[5000][5.0%]lr:[1.00] X R:[0.314] D:[0.059] G:[0.096] WD:[0.000] / Y R:[0.129] D:[0.065] G:[0.081] WD:[0.000]\n",
      "   LC:[0.148] / NCE:[0.000] / X2Y:[0.150] / total_loss:[1.043]\n",
      "[nets/lwl2_wae_x/weights.mat] saved. Size is[0.448]MB.\n",
      "[nets/lwl2_wae_y/weights.mat] saved. Size is[0.436]MB.\n",
      "Checkpoint it:[5000] total_loss:[1.043].\n"
     ]
    }
   ],
   "source": [
    "# Loop\n",
    "lr_rate = 1.0\n",
    "lr_d_weight_x,lr_d_weight_y = 1.0,1.0 # adaptive learning rate \n",
    "total_loss_prev = np.inf\n",
    "for it in range(int(max_iter)): \n",
    "    zero_to_one = it/max_iter\n",
    "    # lr_rate = lr_rate_fr - (lr_rate_fr-lr_rate_to)*zero_to_one\n",
    "    lr_rate = min((it+1e-4)**(-0.5),(it+1e-4)*warmup_it**(-1.5)) / \\\n",
    "        min((warmup_it+1e-4)**(-0.5),(warmup_it+1e-4)*warmup_it**(-1.5))\n",
    "\n",
    "    noise_std = 0.0\n",
    "    # X Recon\n",
    "    r_idx = np.random.permutation(x_recon_in.shape[0])[:batch_size]\n",
    "    x_batch_recon_in = x_recon_in[r_idx,:] + noise_std*np.random.randn(batch_size,xdim) # x_pair_tildes ?\n",
    "    x_batch_recon_out = x_recon_out[r_idx,:]\n",
    "    # X Latent\n",
    "    r_idx = np.random.permutation(x_latent.shape[0])[:batch_size]\n",
    "    x_batch_latent = x_latent[r_idx,:]\n",
    "    # Y Recon\n",
    "    r_idx = np.random.permutation(y_recon_in.shape[0])[:batch_size]\n",
    "    y_batch_recon_in = y_recon_in[r_idx,:] + noise_std*np.random.randn(batch_size,ydim)\n",
    "    y_batch_recon_out = y_recon_out[r_idx,:]\n",
    "    # Y Latent\n",
    "    r_idx = np.random.permutation(y_latent.shape[0])[:batch_size]\n",
    "    y_batch_latent = y_latent[r_idx,:]\n",
    "    # X-Y glue\n",
    "    r_idx = np.random.permutation(x_glue.shape[0])[:batch_size]\n",
    "    x_batch_glue = x_glue[r_idx,:] + noise_std*np.random.randn(batch_size,xdim)\n",
    "    y_batch_glue = y_glue[r_idx,:]\n",
    "    # X->Y mapping\n",
    "    r_idx = np.random.permutation(x_x2y.shape[0])[:batch_size]\n",
    "    x_batch_x2y = x_x2y[r_idx,:] + noise_std*np.random.randn(batch_size,xdim)\n",
    "    y_batch_x2y = y_x2y[r_idx,:]\n",
    "    \n",
    "    # Update\n",
    "    recon_loss_x,wd_loss_x,d_loss_x,g_loss_x,\\\n",
    "        recon_loss_y,wd_loss_y,d_loss_y,g_loss_y,\\\n",
    "        lc_loss,nce_loss,x2y_loss = S.update(\n",
    "            sess,\n",
    "            x_recon_in=x_batch_recon_in,x_recon_out=x_batch_recon_out,x_latent=x_batch_latent,\n",
    "            y_recon_in=y_batch_recon_in,y_recon_out=y_batch_recon_out,y_latent=y_batch_latent,\n",
    "            x_glue=x_batch_glue,y_glue=y_batch_glue,\n",
    "            x_x2y=x_batch_x2y,y_x2y=y_batch_x2y,\n",
    "            latent_beta=latent_beta,l1_recon_coef=l1_recon_coef,l2_recon_coef=l2_recon_coef,wd_coef=wd_coef,\n",
    "            lr_recon_x=lr_recon,lr_d_x=lr_d*lr_d_weight_x,lr_g_x=lr_g,\n",
    "            lr_recon_y=lr_recon,lr_d_y=lr_d*lr_d_weight_y,lr_g_y=lr_g,\n",
    "            lr_lc=lr_lc,l1_lc_coef=l1_lc_coef,l2_lc_coef=l2_lc_coef,lr_nce=lr_nce,nce_coef=0.0,\n",
    "            lr_x2y=lr_x2y,l1_x2y_coef=l1_x2y_coef,l2_x2y_coef=l2_x2y_coef,lr_rate=lr_rate\n",
    "            )\n",
    "    lr_d_weight_x,lr_d_weight_y = min(1.0,d_loss_x/(0.1+g_loss_x)),min(1.0,d_loss_y/(0.1+g_loss_y))\n",
    "    total_loss = recon_loss_x + wd_loss_x + d_loss_x + g_loss_x + recon_loss_y + wd_loss_y + \\\n",
    "        d_loss_y + g_loss_y + lc_loss + nce_loss + x2y_loss\n",
    "\n",
    "    # Print results every some iterations \n",
    "    if ((it % print_every) == 0) or ((it+1) == max_iter): \n",
    "        print ((\"[%d][%.1f%%]lr:[%.2f] X R:[%.3f] D:[%.3f] G:[%.3f] WD:[%.3f] / Y R:[%.3f] D:[%.3f] G:[%.3f] WD:[%.3f]\\n\"\n",
    "                \"   LC:[%.3f] / NCE:[%.3f] / X2Y:[%.3f] / total_loss:[%.3f]\")%\n",
    "               (it,zero_to_one*100,lr_rate,recon_loss_x,d_loss_x,g_loss_x,wd_loss_x,\n",
    "                recon_loss_y,d_loss_y,g_loss_y,wd_loss_y,\n",
    "                lc_loss,nce_loss,x2y_loss,total_loss))\n",
    "    # Save?\n",
    "    if ((it % save_every) == 0) and (total_loss < total_loss_prev):\n",
    "        total_loss_prev = total_loss\n",
    "        S.W_x.save_to_mat(sess,it=it,suffix='',VERBOSE=False)\n",
    "        S.W_y.save_to_mat(sess,it=it,suffix='',VERBOSE=False)\n",
    "        print (\"Checkpoint it:[%d] total_loss:[%.3f].\"%(it,total_loss))\n",
    "    \n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
