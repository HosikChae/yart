{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train MLP for Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T11:24:53.132770Z",
     "start_time": "2020-10-30T11:24:53.128688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import glob,os\n",
    "import numpy as np\n",
    "import tensorflow as tf, torch\n",
    "import scipy.io as sio\n",
    "from inspect import isfunction\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T11:11:53.745532Z",
     "start_time": "2020-10-30T11:11:51.673321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have [12] mat files.\n",
      "n:[120000], xdim:[150], ydim:[21].\n"
     ]
    }
   ],
   "source": [
    "model_name = 'alphred' # alphred / panda\n",
    "math_paths = glob.glob('../data/%s-dataset/*.mat'%(model_name))\n",
    "print (\"We have [%d] mat files.\"%(len(math_paths)))\n",
    "for m_idx,mat_path in enumerate(math_paths):\n",
    "    l = sio.loadmat(mat_path) \n",
    "    if m_idx == 0:\n",
    "        in_q,out_t,out_w = l['in_q'],l['out_t'],l['out_w']\n",
    "    else:\n",
    "        in_q = np.concatenate((in_q,l['in_q']),axis=0)\n",
    "        out_t = np.concatenate((out_t,l['out_t']),axis=0)\n",
    "        out_w = np.concatenate((out_w,l['out_w']),axis=0)\n",
    "\n",
    "x_data = in_q;\n",
    "y_data = np.concatenate((out_t,out_w),axis=1)\n",
    "n,xdim,ydim = x_data.shape[0],x_data.shape[1],y_data.shape[1]\n",
    "print (\"n:[%d], xdim:[%d], ydim:[%d].\"%(n,xdim,ydim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T11:11:54.418590Z",
     "start_time": "2020-10-30T11:11:54.386552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "class MultiLayerPerceptionClass(object):\n",
    "    \"\"\"\n",
    "    MLP for Regression\n",
    "    \"\"\"\n",
    "    def __init__(self,name='mlp',xdim=2,ydim=1,hdims=[64]*2,actv=tf.nn.relu,\n",
    "                 adam_beta1=0.5,adam_beta2=0.9,adam_epsilon=1e-0,):\n",
    "        self.name = name\n",
    "        self.xdim = xdim\n",
    "        self.ydim = ydim\n",
    "        self.hdims = hdims\n",
    "        self.actv = actv\n",
    "        \n",
    "        self.adam_beta1 = adam_beta1\n",
    "        self.adam_beta2 = adam_beta2\n",
    "        self.adam_epsilon = adam_epsilon\n",
    "        \n",
    "        with tf.variable_scope(self.name,reuse=False):\n",
    "            self.build_graph()\n",
    "    \n",
    "    def build_graph(self):\n",
    "        ki = tf.contrib.layers.variance_scaling_initializer() # tf.contrib.layers.xavier_initializer()\n",
    "        bi = tf.constant_initializer(value=0)\n",
    "        self.ph_x = tf.placeholder(shape=[None,self.xdim],dtype=tf.float32,name='ph_x') # [n x x_dim]\n",
    "        self.ph_y_trgt = tf.placeholder(shape=[None,self.ydim],dtype=tf.float32,name='ph_y_trgt') # [n x y_dim]\n",
    "        \n",
    "        # Encoder netowrk F: ph_x => y_pred\n",
    "        with tf.variable_scope('F',reuse=False):\n",
    "            net = self.ph_x\n",
    "            for h_idx,hdim in enumerate(self.hdims):\n",
    "                net = tf.layers.dense(net,hdim,activation=self.actv,\n",
    "                                      kernel_initializer=ki,bias_initializer=bi,\n",
    "                                      name='hid_lin_%d'%(h_idx))\n",
    "            self.y_pred = tf.layers.dense(net,self.ydim,activation=None,\n",
    "                                          kernel_initializer=ki,bias_initializer=bi,\n",
    "                                          name='y_pred') # [n x ydim]\n",
    "        # Loss\n",
    "        self.l1_coef = tf.placeholder(shape=[],dtype=tf.float32,name='l1_coef') # [1]\n",
    "        self.l2_coef = tf.placeholder(shape=[],dtype=tf.float32,name='l2_coef') # [1]\n",
    "        self.l1_losses = tf.reduce_sum(tf.abs(self.ph_y_trgt-self.y_pred),axis=1) # [N]\n",
    "        self.l2_losses = tf.reduce_sum((self.ph_y_trgt-self.y_pred)**2,axis=1) # [N]\n",
    "        self.reg_losses = self.l1_coef*self.l1_losses + self.l2_coef*self.l2_losses\n",
    "        self.reg_loss = tf.reduce_mean(self.reg_losses) # [1]\n",
    "        \n",
    "        # Optimizer\n",
    "        self.t_vars = [var for var in tf.trainable_variables() if '%s/'%(self.name) in var.name] \n",
    "        self.lr = tf.placeholder(shape=[],dtype=tf.float32,name='lr') # [1]\n",
    "        self.optm = tf.train.AdamOptimizer(\n",
    "                self.lr,\n",
    "                beta1=self.adam_beta1,beta2=self.adam_beta2,epsilon=self.adam_epsilon).minimize(\n",
    "                self.reg_loss,\n",
    "                var_list=self.t_vars,name='optm') # X encoder + Y decoder\n",
    "        \n",
    "    def update(self,sess,x_batch,y_batch,l1_coef=1.0,l2_coef=1.0,lr=1e-3):\n",
    "        \"\"\"\n",
    "        Update\n",
    "        \"\"\"\n",
    "        feeds = {self.ph_x:x_batch,self.ph_y_trgt:y_batch,self.l1_coef:l1_coef,self.l2_coef:l2_coef,self.lr:lr}\n",
    "        _,loss_val = sess.run([self.optm,self.reg_loss],feed_dict=feeds)\n",
    "        return loss_val\n",
    "\n",
    "    # Save network information to matfile \n",
    "    def save_to_mat(self,sess,epoch=0,suffix='',SAVE_MAT=True,CHANGE_VAR_NAME=True,VERBOSE=True):\n",
    "        \"\"\"\n",
    "            Save to a mat file \n",
    "        \"\"\"\n",
    "        v_names,d = [],{}\n",
    "        c_name = self.name\n",
    "        t_vars = self.t_vars # trainable variables\n",
    "        for v_idx,var in enumerate(t_vars):\n",
    "            w_name,v_name = var.name,var.name\n",
    "            v_shape = var.get_shape().as_list()\n",
    "            if CHANGE_VAR_NAME:\n",
    "                v_name = v_name.replace('/','_') # replace '/' => '_'\n",
    "                v_name = v_name.replace(':','_') # replace ':' => '_'\n",
    "                v_name = v_name.replace('%s_'%(c_name),'') # remove class name\n",
    "                v_name = (v_name[::-1].split('_',1)[1])[::-1] \n",
    "                # remove characters after LAST '_' (hid_0_kernel_0 -> hid_0_kernel)\n",
    "            if ('kernel:' in w_name) or ('bias:' in w_name) or \\\n",
    "                ('moving_mean:' in w_name) or ('moving_variance:' in w_name) or \\\n",
    "                ('gamma:' in w_name) or ('beta:' in w_name): \n",
    "                v_names.append(v_name)\n",
    "                v_val = sess.run(var)\n",
    "                d[v_name] = v_val\n",
    "                \n",
    "        # Class properties\n",
    "        props = ['name','xdim','ydim','hdims','actv']\n",
    "        for prop in props:\n",
    "            if (isfunction(getattr(self,prop))): # function name\n",
    "                d[prop] = getattr(self,prop).__name__\n",
    "            else: # others\n",
    "                d[prop] = getattr(self,prop)\n",
    "                \n",
    "        # Validation data\n",
    "        props = ['x_vald','y_vald']\n",
    "        n_vald = 10\n",
    "        x_vald = np.random.randn(n_vald,self.xdim)\n",
    "        y_vald = sess.run(self.y_pred,feed_dict={self.ph_x:x_vald})\n",
    "        for prop in props:\n",
    "            d[prop] = vars()[prop]\n",
    "            \n",
    "        # Check names and types of things to save\n",
    "        for k_idx,key in enumerate(d.keys()):\n",
    "            item_type = type(d[key]).__name__\n",
    "            if VERBOSE:\n",
    "                print (\"  [%02d] Name:[%s] Type:[%s].\"%(k_idx,key,item_type))\n",
    "        \n",
    "        # Save to a mat file\n",
    "        if SAVE_MAT:\n",
    "            dir_path = 'nets/%s'%(self.name)\n",
    "            mat_path = os.path.join(dir_path,'weights%s.mat'%(suffix))\n",
    "            if not os.path.exists(dir_path):\n",
    "                os.makedirs(dir_path)\n",
    "                print (\"[%s] created.\"%(dir_path))\n",
    "            sio.savemat(mat_path,d) # save to a mat file\n",
    "            print (\"[%s] saved. Size is[%.3f]MB.\"%(mat_path,os.path.getsize(mat_path) / 1000000))\n",
    "            \n",
    "def gpu_sess(): \n",
    "    config = tf.ConfigProto(); \n",
    "    config.gpu_options.allow_growth=True\n",
    "    sess = tf.Session(config=config)\n",
    "    return sess        \n",
    "        \n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### PyTorch Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T11:24:43.427138Z",
     "start_time": "2020-10-30T11:24:43.386901Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MultiLayerPerceptionClass(torch.nn.Module):\n",
    "    def __init__(self, name='mlp', xdim=2, ydim=1, hdims=[64]*2, actv=torch.nn.ReLU, x_mean=0.0, x_std=1.0, y_mean=0.0, y_std=1.0, adam_beta1=0.5, adam_beta2=0.9,adam_epsilon=1e-0, input_normalization=True, output_normalization=True, batch_normalization=False):\n",
    "        super(MultiLayerPerceptionClass, self).__init__()\n",
    "        self.name = name\n",
    "        self.xdim = xdim\n",
    "        self.ydim = ydim\n",
    "        self.hdims = hdims\n",
    "        self.adam_beta1 = adam_beta1\n",
    "        self.adam_beta2 = adam_beta2\n",
    "        self.adam_epsilon=adam_epsilon\n",
    "        self.x_mean = x_mean\n",
    "        self.x_std = x_std\n",
    "        self.y_mean = y_mean\n",
    "        self.y_std = y_std\n",
    "        \n",
    "        self.actv = actv\n",
    "        \n",
    "        self.input_normalization = input_normalization\n",
    "        self.output_normalization = output_normalization\n",
    "        self.batch_normalization = batch_normalization\n",
    "        \n",
    "        if self.batch_normalization:\n",
    "            self.bns = torch.nn.ModuleList([torch.nn.BatchNorm1d(self.hdims[0]), ])\n",
    "        self.fcs = torch.nn.ModuleList([torch.nn.Linear(self.xdim, self.hdims[0])]) #, bias=False) ]) # bias is redundant when with BN\n",
    "\n",
    "        for hdx in range(1, len(self.hdims)):\n",
    "            if self.batch_normalization: self.bns.append(torch.nn.BatchNorm1d(self.hdims[hdx]))\n",
    "            self.fcs.append(torch.nn.Linear(self.hdims[hdx-1], self.hdims[hdx])) #, bias=False)) # bias is redundant when with BN\n",
    "\n",
    "        self.out = torch.nn.Linear(self.hdims[-1], ydim)\n",
    "\n",
    "        self.criterion = self.l1_l2_loss # torch.nn.MSELoss(reduction='sum')\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), betas=(self.adam_beta1, self.adam_beta2), eps=self.adam_epsilon)\n",
    "\n",
    "        # self.es = util.EarlyStopping(patience=5)\n",
    "#         self.lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', patience=0, factor=0.5, min_lr=1e-6)\n",
    "\n",
    "\n",
    "    def l1_l2_loss(self, pred, target, l1_coef, l2_coef):\n",
    "        l1_loss = torch.sum(torch.abs(pred-target), axis=1)\n",
    "        l2_loss = torch.sum((pred - target)**2, axis=1)\n",
    "        loss = l1_coef * l1_loss + l2_coef * l2_loss       \n",
    "        return loss.mean()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.input_normalization:\n",
    "            x = self.normalize(x, self.x_mean, self.x_std)\n",
    "    \n",
    "        for ldx, fc in enumerate(self.fcs):\n",
    "            if self.batch_normalization:\n",
    "                x = self.actv()(self.bns[ldx](fc(x)))\n",
    "            else:\n",
    "                x = self.actv()(fc(x))\n",
    "\n",
    "        x = self.out(x)\n",
    "\n",
    "        # x = self.denormalize(x, self.y_mean, self.y_std)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def normalize(self, x, mean, std):\n",
    "        return (x - mean)/std\n",
    "\n",
    "    def denormalize(self, x, mean, std):\n",
    "        return std * x + mean\n",
    "\n",
    "    def y_denormalize(self, y_normalized):\n",
    "        return self.denormalize(y_normalized, self.y_mean, self.y_std)\n",
    "\n",
    "    @classmethod\n",
    "    def model_from_checkpoint(cls, ckpt):\n",
    "        model = cls(xdim=ckpt['xdim'], ydim=ckpt['ydim'], hdims=ckpt['hdims'], lr=ckpt['lr'], x_mean=ckpt['x_mean'], x_std=ckpt['x_std'], y_mean=ckpt['y_mean'], y_std=ckpt['y_std'])\n",
    "        model.load_state_dict(state_dict=ckpt['model_state_dict'])\n",
    "        model.optimizer.load_state_dict(state_dict=ckpt['optimizer_state_dict'])\n",
    "\n",
    "        return model    \n",
    "    \n",
    "    def update(self, sess, x_batch,y_batch,l1_coef=1.0,l2_coef=1.0,lr=1e-3, validation=False):\n",
    "        \"\"\"\n",
    "        Update\n",
    "        \"\"\"\n",
    "        \n",
    "        # update learning rate\n",
    "        for g in self.optimizer.param_groups:\n",
    "            g['lr'] = lr\n",
    "\n",
    "        x_batch = torch.tensor(x_batch).float()\n",
    "        y_batch = torch.tensor(y_batch).float()\n",
    "        \n",
    "        if validation:    \n",
    "            self.eval()\n",
    "            y_pred = self.forward(x_batch)\n",
    "            if self.output_normalization:\n",
    "                loss = self.criterion(pred=y_pred, target=self.normalize(y_batch, self.y_mean, self.y_std), l1_coef=l1_coef, l2_coef=l2_coef)\n",
    "            else:\n",
    "                loss = self.criterion(pred=y_pred, target=y_batch, l1_coef=l1_coef, l2_coef=l2_coef)\n",
    "                \n",
    "#             if validation:\n",
    "#                 self.lr_scheduler.step(loss.item())\n",
    "        else:\n",
    "            self.train()\n",
    "            y_pred = self.forward(x_batch)\n",
    "            if self.output_normalization:\n",
    "                loss = self.criterion(pred=y_pred, target=self.normalize(y_batch, self.y_mean, self.y_std), l1_coef=l1_coef, l2_coef=l2_coef)\n",
    "            else:\n",
    "                loss = self.criterion(pred=y_pred, target=y_batch, l1_coef=l1_coef, l2_coef=l2_coef)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    # Save network information to matfile \n",
    "    def save_to_mat(self,sess,epoch=0,suffix='',SAVE_MAT=True,CHANGE_VAR_NAME=True,VERBOSE=True):\n",
    "        \"\"\"\n",
    "            Save to a mat file \n",
    "        \"\"\"\n",
    "        \n",
    "        torch_keys = ['weight', 'bias']\n",
    "        tf_keys = ['kernel', 'bias']\n",
    "        \n",
    "        v_names,d = [],{}\n",
    "        c_name = self.name\n",
    "        t_vars = self.state_dict() # trainable variables\n",
    "\n",
    "        for v_idx, (k, v) in enumerate(t_vars.items()):\n",
    "            if 'fcs' in k:\n",
    "                prefix = 'F_hid_lin_'\n",
    "            elif 'out' in k:\n",
    "                prefix = 'F_y_pred_'\n",
    "            else:\n",
    "                raise Exception('unexpected field')\n",
    "\n",
    "            n_right_trim = 0\n",
    "\n",
    "            for kdx, key in enumerate(torch_keys): # (w_name -> v_name): ('mlp_alphred/F/hid_lin_0/kernel:0' -> F_hid_lin_0_kernel') # 'mlp_alphred/F/y_pred/kernel:0' \n",
    "                if key in k:\n",
    "                    n_right_trim = -len(key)-1\n",
    "                    try:\n",
    "                        ldx_str = f\"{int(k.split('.')[1])}_\"\n",
    "                    except:\n",
    "                        ldx_str = ''\n",
    "                    v_name = f\"{prefix}{ldx_str}{tf_keys[kdx]}\"    \n",
    "                    print(v_name)\n",
    "\n",
    "                    v_names.append(v_name)\n",
    "                    \n",
    "                    if kdx == 0:\n",
    "                        d[v_name] = v.detach().numpy().T\n",
    "                    else:\n",
    "                        d[v_name] = v.detach().numpy()\n",
    "        \n",
    "        # Class properties\n",
    "        props = ['name','xdim','ydim','hdims','actv']\n",
    "        for prop in props:\n",
    "            if prop == 'actv':\n",
    "                d[prop] = getattr(self,prop).__name__.lower()\n",
    "            elif (isfunction(getattr(self,prop))): # function name\n",
    "                d[prop] = getattr(self,prop).__name__\n",
    "            else: # others\n",
    "                d[prop] = getattr(self,prop)\n",
    "                \n",
    "        # Validation data\n",
    "        props = ['x_vald','y_vald']\n",
    "        n_vald = 10\n",
    "        x_vald = np.random.randn(n_vald, self.xdim)\n",
    "        y_vald = self.forward(torch.tensor(x_vald).float()).detach().numpy()\n",
    "        for prop in props:\n",
    "            d[prop] = vars()[prop]\n",
    "            \n",
    "        # Check names and types of things to save\n",
    "        for k_idx,key in enumerate(d.keys()):\n",
    "            item_type = type(d[key]).__name__\n",
    "            if VERBOSE:\n",
    "                print (\"  [%02d] Name:[%s] Type:[%s].\"%(k_idx,key,item_type))\n",
    "        \n",
    "        # Save to a mat file\n",
    "        if SAVE_MAT:\n",
    "            dir_path = 'nets/%s'%(self.name)\n",
    "            mat_path = os.path.join(dir_path,'weights%s.mat'%(suffix))\n",
    "            if not os.path.exists(dir_path):\n",
    "                os.makedirs(dir_path)\n",
    "                print (\"[%s] created.\"%(dir_path))\n",
    "            sio.savemat(mat_path,d) # save to a mat file\n",
    "            print (\"[%s] saved. Size is[%.3f]MB.\"%(mat_path,os.path.getsize(mat_path) / 1000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T11:23:47.037802Z",
     "start_time": "2020-10-30T11:23:47.033605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "hdims = [256]*3\n",
    "actv = tf.nn.relu # torch.nn.ReLU # for pytorch \n",
    "l1_coef,l2_coef = 1.0,5.0\n",
    "lr = 1e-3\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T11:23:19.577583Z",
     "start_time": "2020-10-30T11:23:10.290967Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "[0/937][0.00] loss_val:[22.002].\n",
      "[500/937][0.53] loss_val:[4.951].\n",
      "[936/937][1.00] loss_val:[4.829].\n",
      "F_hid_lin_0_kernel\n",
      "F_hid_lin_0_bias\n",
      "F_hid_lin_1_kernel\n",
      "F_hid_lin_1_bias\n",
      "F_hid_lin_2_kernel\n",
      "F_hid_lin_2_bias\n",
      "F_y_pred_kernel\n",
      "F_y_pred_bias\n",
      "  [00] Name:[F_hid_lin_0_kernel] Type:[ndarray].\n",
      "  [01] Name:[F_hid_lin_0_bias] Type:[ndarray].\n",
      "  [02] Name:[F_hid_lin_1_kernel] Type:[ndarray].\n",
      "  [03] Name:[F_hid_lin_1_bias] Type:[ndarray].\n",
      "  [04] Name:[F_hid_lin_2_kernel] Type:[ndarray].\n",
      "  [05] Name:[F_hid_lin_2_bias] Type:[ndarray].\n",
      "  [06] Name:[F_y_pred_kernel] Type:[ndarray].\n",
      "  [07] Name:[F_y_pred_bias] Type:[ndarray].\n",
      "  [08] Name:[name] Type:[str].\n",
      "  [09] Name:[xdim] Type:[int].\n",
      "  [10] Name:[ydim] Type:[int].\n",
      "  [11] Name:[hdims] Type:[list].\n",
      "  [12] Name:[actv] Type:[str].\n",
      "  [13] Name:[x_vald] Type:[ndarray].\n",
      "  [14] Name:[y_vald] Type:[ndarray].\n",
      "[nets/mlp_alphred/weights.mat] saved. Size is[0.717]MB.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "M = MultiLayerPerceptionClass(\n",
    "    name='mlp_%s'%(model_name),xdim=xdim,ydim=ydim,hdims=hdims,actv=actv)\n",
    "sess = gpu_sess()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print (\"Done.\")\n",
    "\n",
    "n_epoch,batch_size = 1,128\n",
    "# n_epoch,batch_size = 10,128\n",
    "max_iter,print_every = int(n*n_epoch/batch_size),500\n",
    "for it in range(int(max_iter)): \n",
    "    zero_to_one = it/max_iter\n",
    "    one_to_zero = 1.0-zero_to_one # for lr schedule \n",
    "    r_idx = np.random.permutation(n)[:batch_size]\n",
    "    x_batch,y_batch = x_data[r_idx,:],y_data[r_idx,:]\n",
    "    \n",
    "    # Update\n",
    "    loss_val = M.update(\n",
    "        sess=sess,x_batch=x_batch,y_batch=y_batch,l1_coef=l1_coef,l2_coef=l2_coef,lr=lr*one_to_zero)\n",
    "    \n",
    "    # Print results every some iterations \n",
    "    if ((it % print_every) == 0) or ((it+1) == max_iter):         \n",
    "        print (\"[%d/%d][%.2f] loss_val:[%.3f].\"%(it,max_iter,zero_to_one,loss_val))\n",
    "        \n",
    "# Save\n",
    "M.save_to_mat(sess)\n",
    "print (\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
