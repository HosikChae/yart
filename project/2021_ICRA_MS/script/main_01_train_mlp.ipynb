{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train MLP for Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import glob,os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.io as sio\n",
    "from inspect import isfunction\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have [12] mat files.\n",
      "n:[120000], xdim:[150], ydim:[21].\n"
     ]
    }
   ],
   "source": [
    "model_name = 'alphred' # alphred / panda\n",
    "math_paths = glob.glob('../data/%s-dataset/*.mat'%(model_name))\n",
    "print (\"We have [%d] mat files.\"%(len(math_paths)))\n",
    "for m_idx,mat_path in enumerate(math_paths):\n",
    "    l = sio.loadmat(mat_path) \n",
    "    if m_idx == 0:\n",
    "        in_q,out_t,out_w = l['in_q'],l['out_t'],l['out_w']\n",
    "    else:\n",
    "        in_q = np.concatenate((in_q,l['in_q']),axis=0)\n",
    "        out_t = np.concatenate((out_t,l['out_t']),axis=0)\n",
    "        out_w = np.concatenate((out_w,l['out_w']),axis=0)\n",
    "\n",
    "x_data = in_q;\n",
    "y_data = np.concatenate((out_t,out_w),axis=1)\n",
    "n,xdim,ydim = x_data.shape[0],x_data.shape[1],y_data.shape[1]\n",
    "print (\"n:[%d], xdim:[%d], ydim:[%d].\"%(n,xdim,ydim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "class MultiLayerPerceptionClass(object):\n",
    "    \"\"\"\n",
    "    MLP for Regression\n",
    "    \"\"\"\n",
    "    def __init__(self,name='mlp',xdim=2,ydim=1,hdims=[64]*2,actv=tf.nn.relu,\n",
    "                 adam_beta1=0.5,adam_beta2=0.9,adam_epsilon=1e-0,):\n",
    "        self.name = name\n",
    "        self.xdim = xdim\n",
    "        self.ydim = ydim\n",
    "        self.hdims = hdims\n",
    "        self.actv = actv\n",
    "        \n",
    "        self.adam_beta1 = adam_beta1\n",
    "        self.adam_beta2 = adam_beta2\n",
    "        self.adam_epsilon = adam_epsilon\n",
    "        \n",
    "        with tf.variable_scope(self.name,reuse=False):\n",
    "            self.build_graph()\n",
    "    \n",
    "    def build_graph(self):\n",
    "        ki = tf.contrib.layers.variance_scaling_initializer() # tf.contrib.layers.xavier_initializer()\n",
    "        bi = tf.constant_initializer(value=0)\n",
    "        self.ph_x = tf.placeholder(shape=[None,self.xdim],dtype=tf.float32,name='ph_x') # [n x x_dim]\n",
    "        self.ph_y_trgt = tf.placeholder(shape=[None,self.ydim],dtype=tf.float32,name='ph_y_trgt') # [n x y_dim]\n",
    "        \n",
    "        # Encoder netowrk F: ph_x => y_pred\n",
    "        with tf.variable_scope('F',reuse=False):\n",
    "            net = self.ph_x\n",
    "            for h_idx,hdim in enumerate(self.hdims):\n",
    "                net = tf.layers.dense(net,hdim,activation=self.actv,\n",
    "                                      kernel_initializer=ki,bias_initializer=bi,\n",
    "                                      name='hid_lin_%d'%(h_idx))\n",
    "            self.y_pred = tf.layers.dense(net,self.ydim,activation=None,\n",
    "                                          kernel_initializer=ki,bias_initializer=bi,\n",
    "                                          name='y_pred') # [n x ydim]\n",
    "        # Loss\n",
    "        self.l1_coef = tf.placeholder(shape=[],dtype=tf.float32,name='l1_coef') # [1]\n",
    "        self.l2_coef = tf.placeholder(shape=[],dtype=tf.float32,name='l2_coef') # [1]\n",
    "        self.l1_losses = tf.reduce_sum(tf.abs(self.ph_y_trgt-self.y_pred),axis=1) # [N]\n",
    "        self.l2_losses = tf.reduce_sum((self.ph_y_trgt-self.y_pred)**2,axis=1) # [N]\n",
    "        self.reg_losses = self.l1_coef*self.l1_losses + self.l2_coef*self.l2_losses\n",
    "        self.reg_loss = tf.reduce_mean(self.reg_losses) # [1]\n",
    "        \n",
    "        # Optimizer\n",
    "        self.t_vars = [var for var in tf.trainable_variables() if '%s/'%(self.name) in var.name] \n",
    "        self.lr = tf.placeholder(shape=[],dtype=tf.float32,name='lr') # [1]\n",
    "        self.optm = tf.train.AdamOptimizer(\n",
    "                self.lr,\n",
    "                beta1=self.adam_beta1,beta2=self.adam_beta2,epsilon=self.adam_epsilon).minimize(\n",
    "                self.reg_loss,\n",
    "                var_list=self.t_vars,name='optm') # X encoder + Y decoder\n",
    "        \n",
    "    def update(self,sess,x_batch,y_batch,l1_coef=1.0,l2_coef=1.0,lr=1e-3):\n",
    "        \"\"\"\n",
    "        Update\n",
    "        \"\"\"\n",
    "        feeds = {self.ph_x:x_batch,self.ph_y_trgt:y_batch,self.l1_coef:l1_coef,self.l2_coef:l2_coef,self.lr:lr}\n",
    "        _,loss_val = sess.run([self.optm,self.reg_loss],feed_dict=feeds)\n",
    "        return loss_val\n",
    "\n",
    "    # Save network information to matfile \n",
    "    def save_to_mat(self,sess,epoch=0,suffix='',SAVE_MAT=True,CHANGE_VAR_NAME=True,VERBOSE=True):\n",
    "        \"\"\"\n",
    "            Save to a mat file \n",
    "        \"\"\"\n",
    "        v_names,d = [],{}\n",
    "        c_name = self.name\n",
    "        t_vars = self.t_vars # trainable variables\n",
    "        for v_idx,var in enumerate(t_vars):\n",
    "            w_name,v_name = var.name,var.name\n",
    "            v_shape = var.get_shape().as_list()\n",
    "            if CHANGE_VAR_NAME:\n",
    "                v_name = v_name.replace('/','_') # replace '/' => '_'\n",
    "                v_name = v_name.replace(':','_') # replace ':' => '_'\n",
    "                v_name = v_name.replace('%s_'%(c_name),'') # remove class name\n",
    "                v_name = (v_name[::-1].split('_',1)[1])[::-1] \n",
    "                # remove characters after LAST '_' (hid_0_kernel_0 -> hid_0_kernel)\n",
    "            if ('kernel:' in w_name) or ('bias:' in w_name) or \\\n",
    "                ('moving_mean:' in w_name) or ('moving_variance:' in w_name) or \\\n",
    "                ('gamma:' in w_name) or ('beta:' in w_name): \n",
    "                v_names.append(v_name)\n",
    "                v_val = sess.run(var)\n",
    "                d[v_name] = v_val\n",
    "                \n",
    "        # Class properties\n",
    "        props = ['name','xdim','ydim','hdims','actv']\n",
    "        for prop in props:\n",
    "            if (isfunction(getattr(self,prop))): # function name\n",
    "                d[prop] = getattr(self,prop).__name__\n",
    "            else: # others\n",
    "                d[prop] = getattr(self,prop)\n",
    "                \n",
    "        # Validation data\n",
    "        props = ['x_vald','y_vald']\n",
    "        n_vald = 10\n",
    "        x_vald = np.random.randn(n_vald,self.xdim)\n",
    "        y_vald = sess.run(self.y_pred,feed_dict={self.ph_x:x_vald})\n",
    "        for prop in props:\n",
    "            d[prop] = vars()[prop]\n",
    "            \n",
    "        # Check names and types of things to save\n",
    "        for k_idx,key in enumerate(d.keys()):\n",
    "            item_type = type(d[key]).__name__\n",
    "            if VERBOSE:\n",
    "                print (\"  [%02d] Name:[%s] Type:[%s].\"%(k_idx,key,item_type))\n",
    "        \n",
    "        # Save to a mat file\n",
    "        if SAVE_MAT:\n",
    "            dir_path = 'nets/%s'%(self.name)\n",
    "            mat_path = os.path.join(dir_path,'weights%s.mat'%(suffix))\n",
    "            if not os.path.exists(dir_path):\n",
    "                os.makedirs(dir_path)\n",
    "                print (\"[%s] created.\"%(dir_path))\n",
    "            sio.savemat(mat_path,d) # save to a mat file\n",
    "            print (\"[%s] saved. Size is[%.3f]MB.\"%(mat_path,os.path.getsize(mat_path) / 1000000))\n",
    "            \n",
    "def gpu_sess(): \n",
    "    config = tf.ConfigProto(); \n",
    "    config.gpu_options.allow_growth=True\n",
    "    sess = tf.Session(config=config)\n",
    "    return sess        \n",
    "        \n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "hdims = [256]*3\n",
    "actv = tf.nn.relu\n",
    "l1_coef,l2_coef = 1.0,5.0\n",
    "lr = 1e-3\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "[0/9375][0.00] loss_val:[187.710].\n",
      "[500/9375][0.05] loss_val:[4.164].\n",
      "[1000/9375][0.11] loss_val:[3.348].\n",
      "[1500/9375][0.16] loss_val:[3.213].\n",
      "[2000/9375][0.21] loss_val:[2.805].\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "M = MultiLayerPerceptionClass(\n",
    "    name='mlp_%s'%(model_name),xdim=xdim,ydim=ydim,hdims=hdims,actv=actv)\n",
    "sess = gpu_sess()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print (\"Done.\")\n",
    "\n",
    "n_epoch,batch_size = 10,128\n",
    "max_iter,print_every = int(n*n_epoch/batch_size),500\n",
    "for it in range(int(max_iter)): \n",
    "    zero_to_one = it/max_iter\n",
    "    one_to_zero = 1.0-zero_to_one # for lr schedule \n",
    "    r_idx = np.random.permutation(n)[:batch_size]\n",
    "    x_batch,y_batch = x_data[r_idx,:],y_data[r_idx,:]\n",
    "    \n",
    "    # Update\n",
    "    loss_val = M.update(\n",
    "        sess=sess,x_batch=x_batch,y_batch=y_batch,l1_coef=l1_coef,l2_coef=l2_coef,lr=lr*one_to_zero)\n",
    "    \n",
    "    # Print results every some iterations \n",
    "    if ((it % print_every) == 0) or ((it+1) == max_iter):         \n",
    "        print (\"[%d/%d][%.2f] loss_val:[%.3f].\"%(it,max_iter,zero_to_one,loss_val))\n",
    "        \n",
    "# Save\n",
    "M.save_to_mat(sess)\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
